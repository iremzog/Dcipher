# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uQ4AM1rh8MzwFPo0Xc14lris3Qk3HYnh

#Text Classifier

##Imports
"""

from google.colab import drive

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_hub as hub
# import tensorflow_text as text
from tensorflow.keras.layers import Input, Dense, Dropout, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping

# from keras.preprocessing.text import Tokenizer
# from keras.preprocessing.sequence import pad_sequences
# from tf.keras import preprocessing
# from tf.keras.layers.experimental.preprocessing import TextVectorization

print(tf.__version__)
print(pd.__version__)
print(np.__version__)
print(hub.__version__)
print(matplotlib.__version__)

drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/Colab Notebooks/Dchiper/'
data_file = root_path + 'data/wos2class.json'
training_file = root_path + 'data/wos2class.train.json'
test_file = root_path + 'data/wos2class.test.json'

"""##Functions and Parameters"""


def dataset_prep(dataset):

    dataset['Label'] = dataset['Label'].astype('category')
    dataset["Target"] = dataset['Label'].cat.codes

    titles = training_set['Title'].to_numpy()
    abstracts = training_set['Abstract'].to_numpy()
    labels = training_set['Target'].to_numpy()

    features = [titles, abstracts]

    return features, labels


"""Takes json formatted data file and splits them into training and
test set then, stores them into training and test files.
Split ratio is the test ratio. Returns training dataset.
"""

seed = 1234


def train_test_split(data_file, training_file, test_file, split=0.2):

    dataframe = pd.read_json(data_file)

    dataframe = dataframe.sample(frac=1, random_state=seed)
    train_size = int(len(dataframe)*(1-split))

    train = dataframe.iloc[:train_size]
    test = dataframe.iloc[train_size:]

    train.to_json(training_file, orient='records')
    test.to_json(test_file, orient='records')

    return train


"""Plotting the word numbers of each text:"""


def plot_histogram(sequence, color, maxlength=50):

    x = np.zeros(len(sequence))
    for i, seq in enumerate(sequence):
        x[i] = len(seq)

    plt.hist(x, bins=50, color=color, range=[0, maxlength])
    plt.show()


"""##Training Data Preparation"""

training_set = train_test_split(data_file,
                                training_file,
                                test_file)
[X_titles, X_abstracts], y = dataset_prep(training_set)

X_titles[0], X_abstracts[0]

"""##Deep Neural Network

Gives accuracy around ~0.67

###For titles:
"""

# title_len = 30
# num_words = 500

# tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')
# tokenizer.fit_on_texts(X_titles)
# title_seqs = tokenizer.texts_to_sequences(X_titles)
# padded_titles = pad_sequences(title_seqs,
#                               padding='post',
#                               truncating='post',
#                               maxlen=title_len)

# plot_histogram(title_seqs, color='springgreen')

"""###For Abstracts:"""

# abst_len = 400

# tokenizer_2 = Tokenizer(num_words=num_words, oov_token='<OOV>')
# tokenizer_2.fit_on_texts(X_abstracts)
# abstract_seqs = tokenizer_2.texts_to_sequences(X_abstracts)
# padded_abstracts = pad_sequences(abstract_seqs,
#                                  padding='post',
#                                  truncating='post',
#                                  maxlen=abst_len)

# plot_histogram(abstract_seqs, maxlength=abst_len, color='lightcoral')

"""###The Network:"""

# embed_output = 128
# poolsize = 5
# dense_1_output = 256
# dense_2_output = 128
# alpha = 0.0001

# # Titles
# title_input=Input(shape=(title_len,))
# embed1=Embedding(input_dim=num_words,
#                  output_dim=embed_output,
#                  input_length=title_len,
#                  embeddings_initializer='glorot_normal',
#                  trainable=True)(title_input)
# pooled1 = GlobalAveragePooling1D()(embed1)

# # Abstracts
# abs_input=Input(shape=(abst_len,))
# embed2=Embedding(input_dim=num_words,
#                  output_dim=embed_output,
#                  input_length=abst_len,
#                  embeddings_initializer='glorot_normal',
#                  trainable=True)(abs_input)
# pooled2 = GlobalAveragePooling1D()(embed2)

# # Merged networks
# merge = concatenate([pooled1, pooled2])
# dense = Dense(dense_1_output,
#               activation='relu')(merge)
# dropped = Dropout(0.33)(dense)
# dense2 = Dense(dense_2_output,activation='relu',
#                kernel_regularizer=regularizers.l2(0.0003))(dropped)
# dropped2 = Dropout(0.33)(dense2)

# final = Dense(1,activation='sigmoid')(dropped2)

# dnn_model = Model(inputs = [title_input,abs_input], outputs = final)
# dnn_model.compile(optimizer= Adam(lr = alpha),
#                       loss = 'binary_crossentropy',
#                       metrics = ['accuracy'])
# print(dnn_model.summary())

# history = dnn_model.fit([padded_titles, padded_abstracts],
#                         y,
#                         epochs=50,
#                         batch_size=512,
#                         validation_split=0.2,
#                         verbose=1)

"""##DNN with Pretrained (Google) Text Embeddings

Hyperparameters and layer sizes:
"""

dense_1_output = 256
dense_2_output = 64
dropratio = 0.33
alpha = 0.0001  # learning rate
batch = 512
ep = 50  # epoch

hub_model = "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2"

"""The Deep Neural Network:"""

# Title Input Layer
input_1 = Input(shape=(), name="Input1", dtype=tf.string)
hub_layer_1 = hub.KerasLayer(hub_model, input_shape=[],
                             dtype=tf.string, trainable=True)(input_1)

# Abstract Input Layer
input_2 = Input(shape=(), name="Input2", dtype=tf.string)
hub_layer_2 = hub.KerasLayer(hub_model, input_shape=[],
                             dtype=tf.string, trainable=True)(input_2)

# Concat
merge = concatenate([hub_layer_1, hub_layer_2])

# Dense Layer
dense = Dense(dense_1_output, activation='relu')(merge)
dropped = Dropout(dropratio)(dense)
dense2 = Dense(dense_2_output,
               activation='relu',
               kernel_regularizer=regularizers.l2(0.0003))(dropped)
dropped2 = Dropout(dropratio)(dense2)
final = Dense(1, activation='sigmoid')(dropped2)

"""Compiling the network:"""

model = Model(inputs=[input_1, input_2], outputs=final)
model.compile(optimizer='Adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
print(model.summary())

es = EarlyStopping(monitor='val_loss')

history = model.fit([X_titles, X_abstracts],
                    y,
                    epochs=ep,
                    batch_size=batch,
                    validation_split=0.2,
                    verbose=1,
                    callbacks=[es])

"""##Bert Classifier"""

# tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
# tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'

# def build_bert_model():
#   text_input = Input(shape=(),
#                      dtype=tf.string,
#                      name='text')
#   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess,
#                                        name='preprocessing')
#   encoder_inputs = preprocessing_layer(text_input)
#   encoder = hub.KerasLayer(tfhub_handle_encoder,
#                            trainable=True,
#                            name='BERT_encoder')
#   outputs = encoder(encoder_inputs)
#   net = outputs['pooled_output']
#   net = Dropout(0.1)(net)
#   net = Dense(1,
#               activation=None,
#               name='classifier')(net)
#   return Model(text_input, net)

# bert_model = build_bert_model()

"""Compilation of model:"""

# bert_model.compile(optimizer='Adam',
#              loss='binary_crossentropy',
#              metrics=['accuracy'])
# print(bert_model.summary())

# es = EarlyStopping(monitor='val_loss')

# history = bert_model.fit(X_titles,
#                          y, epochs=50,
#                          batch_size=128,
#                          validation_split=0.2,
#                          verbose=1,
#                          callbacks=[es])

"""##Plots

Training and validation accuracies over epochs:
"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""Training and validation losses:"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

model.save(root_path+'models')
